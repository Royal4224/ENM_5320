{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "PyTorchFDM.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Royal4224/ENM_5320/blob/main/Code/PyTorchFDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmZ9U8QwJBTD",
        "outputId": "8943dcc5-f1cf-4c40-c3ef-4637ec8af0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 934.7374 [-1.6706849 -0.5000951  1.66178  ]\n",
            "Epoch [11/1000], Loss: 576.9005 [-1.5739075 -0.4039905  1.7568829]\n",
            "Epoch [21/1000], Loss: 479.4702 [-1.4953773  -0.33122113  1.8204918 ]\n",
            "Epoch [31/1000], Loss: 477.7966 [-1.4498906  -0.30265248  1.8234059 ]\n",
            "Epoch [41/1000], Loss: 459.9402 [-1.4304941  -0.30993524  1.7802165 ]\n",
            "Epoch [51/1000], Loss: 441.6493 [-1.4137815 -0.3239203  1.7272108]\n",
            "Epoch [61/1000], Loss: 427.4574 [-1.384915   -0.32605287  1.6845527 ]\n",
            "Epoch [71/1000], Loss: 411.6237 [-1.3462449 -0.3183057  1.6501122]\n",
            "Epoch [81/1000], Loss: 395.7258 [-1.3061723  -0.31053922  1.6136045 ]\n",
            "Epoch [91/1000], Loss: 379.6728 [-1.268007   -0.30666578  1.5713279 ]\n",
            "Epoch [101/1000], Loss: 363.4683 [-1.2298437  -0.30408102  1.5264331 ]\n",
            "Epoch [111/1000], Loss: 347.2924 [-1.1898882  -0.30025446  1.4818002 ]\n",
            "Epoch [121/1000], Loss: 331.2041 [-1.1485986  -0.29552168  1.4372696 ]\n",
            "Epoch [131/1000], Loss: 315.2977 [-1.1069998  -0.29100356  1.3918873 ]\n",
            "Epoch [141/1000], Loss: 299.6387 [-1.0653064 -0.2868144  1.3457255]\n",
            "Epoch [151/1000], Loss: 284.2864 [-1.0233277  -0.28255203  1.2993499 ]\n",
            "Epoch [161/1000], Loss: 269.2892 [-0.9811142  -0.27813935  1.2529744 ]\n",
            "Epoch [171/1000], Loss: 254.6872 [-0.9388868  -0.27373883  1.2065624 ]\n",
            "Epoch [181/1000], Loss: 240.5131 [-0.89677036 -0.26940158  1.1601777 ]\n",
            "Epoch [191/1000], Loss: 226.7928 [-0.8548076 -0.2650797  1.1139706]\n",
            "Epoch [201/1000], Loss: 213.5464 [-0.81306463 -0.26076618  1.0680401 ]\n",
            "Epoch [211/1000], Loss: 200.7886 [-0.7716275 -0.2564897  1.022442 ]\n",
            "Epoch [221/1000], Loss: 188.5298 [-0.7305625  -0.25226077  0.9772422 ]\n",
            "Epoch [231/1000], Loss: 176.7763 [-0.68992054 -0.24807657  0.9325118 ]\n",
            "Epoch [241/1000], Loss: 165.5308 [-0.6497532  -0.24394178  0.8883084 ]\n",
            "Epoch [251/1000], Loss: 154.7928 [-0.61010945 -0.23986429  0.84468037]\n",
            "Epoch [261/1000], Loss: 144.5592 [-0.5710316  -0.23584788  0.8016745 ]\n",
            "Epoch [271/1000], Loss: 134.8244 [-0.5325565  -0.23189501  0.7593338 ]\n",
            "Epoch [281/1000], Loss: 125.5808 [-0.49471864 -0.22800949  0.7176952 ]\n",
            "Epoch [291/1000], Loss: 116.8192 [-0.45754865 -0.22419462  0.67679197]\n",
            "Epoch [301/1000], Loss: 108.5284 [-0.4210735  -0.22045274  0.63665384]\n",
            "Epoch [311/1000], Loss: 100.6965 [-0.38531682 -0.21678603  0.5973073 ]\n",
            "Epoch [321/1000], Loss: 93.3102 [-0.3502997  -0.21319662  0.55877507]\n",
            "Epoch [331/1000], Loss: 86.3553 [-0.3160401  -0.20968625  0.5210769 ]\n",
            "Epoch [341/1000], Loss: 79.8170 [-0.2825535  -0.20625623  0.48422995]\n",
            "Epoch [351/1000], Loss: 73.6801 [-0.24985282 -0.20290786  0.4482483 ]\n",
            "Epoch [361/1000], Loss: 67.9288 [-0.21794881 -0.1996421   0.4131436 ]\n",
            "Epoch [371/1000], Loss: 62.5470 [-0.18684985 -0.19645964  0.3789253 ]\n",
            "Epoch [381/1000], Loss: 57.5188 [-0.15656239 -0.19336109  0.34560013]\n",
            "Epoch [391/1000], Loss: 52.8279 [-0.12709077 -0.19034673  0.31317315]\n",
            "Epoch [401/1000], Loss: 48.4582 [-0.09843764 -0.18741682  0.28164694]\n",
            "Epoch [411/1000], Loss: 44.3937 [-0.07060382 -0.18457127  0.25102255]\n",
            "Epoch [421/1000], Loss: 40.6187 [-0.04358852 -0.18180995  0.221299  ]\n",
            "Epoch [431/1000], Loss: 37.1176 [-0.01738941 -0.17913257  0.19247371]\n",
            "Epoch [441/1000], Loss: 33.8754 [ 0.00799729 -0.17653865  0.16454251]\n",
            "Epoch [451/1000], Loss: 30.8772 [ 0.03257672 -0.17402762  0.13749966]\n",
            "Epoch [461/1000], Loss: 28.1087 [ 0.05635532 -0.17159875  0.11133811]\n",
            "Epoch [471/1000], Loss: 25.5559 [ 0.07934067 -0.16925122  0.08604945]\n",
            "Epoch [481/1000], Loss: 23.2054 [ 0.10154151 -0.1669841   0.06162408]\n",
            "Epoch [491/1000], Loss: 21.0443 [ 0.12296756 -0.16479634  0.03805123]\n",
            "Epoch [501/1000], Loss: 19.0602 [ 0.14362954 -0.1626868   0.01531912]\n",
            "Epoch [511/1000], Loss: 17.2410 [ 0.16353902 -0.16065426 -0.006585  ]\n",
            "Epoch [521/1000], Loss: 15.5756 [ 0.18270841 -0.15869746 -0.02767481]\n",
            "Epoch [531/1000], Loss: 14.0531 [ 0.20115082 -0.15681502 -0.04796471]\n",
            "Epoch [541/1000], Loss: 12.6631 [ 0.21888004 -0.15500553 -0.06746991]\n",
            "Epoch [551/1000], Loss: 11.3960 [ 0.23591043 -0.15326746 -0.08620623]\n",
            "Epoch [561/1000], Loss: 10.2425 [ 0.252257   -0.15159926 -0.10419011]\n",
            "Epoch [571/1000], Loss: 9.1939 [ 0.26793504 -0.14999941 -0.12143853]\n",
            "Epoch [581/1000], Loss: 8.2420 [ 0.2829604  -0.14846623 -0.13796881]\n",
            "Epoch [591/1000], Loss: 7.3791 [ 0.2973492  -0.14699805 -0.1537988 ]\n",
            "Epoch [601/1000], Loss: 6.5980 [ 0.311118   -0.1455932  -0.16894656]\n",
            "Epoch [611/1000], Loss: 5.8919 [ 0.32428336 -0.14424996 -0.18343048]\n",
            "Epoch [621/1000], Loss: 5.2546 [ 0.3368622  -0.14296663 -0.19726913]\n",
            "Epoch [631/1000], Loss: 4.6801 [ 0.3488716  -0.14174141 -0.21048123]\n",
            "Epoch [641/1000], Loss: 4.1630 [ 0.36032856 -0.1405726  -0.22308558]\n",
            "Epoch [651/1000], Loss: 3.6982 [ 0.37125027 -0.1394584  -0.23510107]\n",
            "Epoch [661/1000], Loss: 3.2810 [ 0.38165393 -0.1383971  -0.24654658]\n",
            "Epoch [671/1000], Loss: 2.9070 [ 0.3915566  -0.13738692 -0.25744092]\n",
            "Epoch [681/1000], Loss: 2.5723 [ 0.40097526 -0.13642612 -0.26780283]\n",
            "Epoch [691/1000], Loss: 2.2731 [ 0.40992698 -0.13551301 -0.27765098]\n",
            "Epoch [701/1000], Loss: 2.0061 [ 0.41842842 -0.13464577 -0.2870038 ]\n",
            "Epoch [711/1000], Loss: 1.7681 [ 0.4264962  -0.13382278 -0.29587948]\n",
            "Epoch [721/1000], Loss: 1.5563 [ 0.43414667 -0.1330424  -0.3042961 ]\n",
            "Epoch [731/1000], Loss: 1.3680 [ 0.4413961  -0.13230295 -0.31227142]\n",
            "Epoch [741/1000], Loss: 1.2009 [ 0.44826028 -0.13160281 -0.31982294]\n",
            "Epoch [751/1000], Loss: 1.0528 [ 0.45475483 -0.1309404  -0.32696787]\n",
            "Epoch [761/1000], Loss: 0.9218 [ 0.46089512 -0.13031408 -0.33372304]\n",
            "Epoch [771/1000], Loss: 0.8059 [ 0.46669614 -0.12972234 -0.34010494]\n",
            "Epoch [781/1000], Loss: 0.7037 [ 0.47217247 -0.12916376 -0.3461297 ]\n",
            "Epoch [791/1000], Loss: 0.6136 [ 0.47733843 -0.1286368  -0.351813  ]\n",
            "Epoch [801/1000], Loss: 0.5344 [ 0.48220807 -0.12814009 -0.35717022]\n",
            "Epoch [811/1000], Loss: 0.4647 [ 0.4867949  -0.12767224 -0.36221635]\n",
            "Epoch [821/1000], Loss: 0.4036 [ 0.49111208 -0.12723188 -0.36696592]\n",
            "Epoch [831/1000], Loss: 0.3500 [ 0.4951725  -0.12681767 -0.37143293]\n",
            "Epoch [841/1000], Loss: 0.3031 [ 0.49898854 -0.12642847 -0.37563115]\n",
            "Epoch [851/1000], Loss: 0.2622 [ 0.5025723  -0.12606288 -0.3795737 ]\n",
            "Epoch [861/1000], Loss: 0.2265 [ 0.50593525 -0.1257199  -0.38327354]\n",
            "Epoch [871/1000], Loss: 0.1953 [ 0.5090888  -0.12539819 -0.38674283]\n",
            "Epoch [881/1000], Loss: 0.1682 [ 0.5120437  -0.12509681 -0.38999367]\n",
            "Epoch [891/1000], Loss: 0.1447 [ 0.5148105  -0.12481459 -0.3930374 ]\n",
            "Epoch [901/1000], Loss: 0.1243 [ 0.5173991  -0.12455063 -0.39588526]\n",
            "Epoch [911/1000], Loss: 0.1066 [ 0.5198191  -0.1243038  -0.39854762]\n",
            "Epoch [921/1000], Loss: 0.0913 [ 0.5220799  -0.12407316 -0.40103477]\n",
            "Epoch [931/1000], Loss: 0.0781 [ 0.5241903  -0.1238579  -0.40335658]\n",
            "Epoch [941/1000], Loss: 0.0667 [ 0.526159   -0.12365707 -0.40552235]\n",
            "Epoch [951/1000], Loss: 0.0569 [ 0.5279939  -0.12346993 -0.40754107]\n",
            "Epoch [961/1000], Loss: 0.0485 [ 0.52970296 -0.12329559 -0.4094212 ]\n",
            "Epoch [971/1000], Loss: 0.0412 [ 0.53129345 -0.12313341 -0.41117102]\n",
            "Epoch [981/1000], Loss: 0.0350 [ 0.53277266 -0.12298251 -0.41279826]\n",
            "Epoch [991/1000], Loss: 0.0297 [ 0.53414714 -0.12284233 -0.41431046]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model (in our case, its y = A*x + b)\n",
        "class FiniteDifferenceOperator(nn.Module):\n",
        "    def __init__(self,Nleft,Nright):\n",
        "        super(FiniteDifferenceOperator, self).__init__()\n",
        "        # Number of total nodes in finite difference stencils\n",
        "        self.Nstencil = Nleft + Nright + 1\n",
        "        self.Nleft = Nleft\n",
        "        self.Nright = Nright\n",
        "\n",
        "        # Initialize with random coefficients\n",
        "        self.stencil = torch.nn.Parameter(torch.randn(self.Nstencil))\n",
        "        # self.stencil = torch.from_numpy((0.5/dx)*np.array([1,0,-1]))\n",
        "        # a unit test to see if learnable stencil performs like hard-coded stencil\n",
        "        # from finite difference example\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the finite difference stencil to the gridfunction x, assuming periodic BC\n",
        "        N_nodes = x.shape[0]\n",
        "        f_out = torch.zeros_like(x)\n",
        "        for i in range(N_nodes):\n",
        "          for jj in range(self.Nstencil):\n",
        "            j = jj - self.Nleft\n",
        "            j_withbc = (i+j)%(N_nodes-1)\n",
        "            f_out[i] += self.stencil[jj]*x[j_withbc]\n",
        "\n",
        "        return f_out\n",
        "\n",
        "\n",
        "# Parameters\n",
        "L = 2.0*np.pi  # Length of the domain\n",
        "T = np.pi   # Total time\n",
        "nx = 50  # Number of spatial points\n",
        "nt = 50  # Number of time steps\n",
        "\n",
        "# Discretization\n",
        "dx = L / nx\n",
        "dt = T / nt\n",
        "x = np.linspace(0, L, nx)\n",
        "u = torch.from_numpy(np.sin(2 * np.pi * x / L))  # Initial condition\n",
        "uexact = np.sin(2 * np.pi * (x-T) / L)  # Exact solution\n",
        "Dx = FiniteDifferenceOperator(1,1)  # Finite difference operator w a neighbor on either side\n",
        "\n",
        "def uexact(x,t):\n",
        "  return torch.from_numpy(np.sin(2 * np.pi * (x-t) / L))\n",
        "\n",
        "# Define the optimizer so that it optmizes over stencil parameters\n",
        "optimizer = optim.Adam(Dx.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    loss = 0\n",
        "    for n in range(nt):\n",
        "        t = n*dt\n",
        "        un = uexact(x,t)\n",
        "        unp1 = uexact(x,t+dt)\n",
        "        dudtn = (unp1-un)/dt\n",
        "        dudt_learned = Dx(un)/dx\n",
        "        loss = loss + torch.mean((dudtn-dudt_learned)**2)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}', Dx.stencil.detach().numpy())\n",
        "\n",
        "# Plot the results\n",
        "# for n in range(nt):\n",
        "#     u_new = np.zeros_like(u)\n",
        "#     for i in range(nx):\n",
        "#         u_new[i] = u[i] + ( 0.5 * dt / dx) * (u[(i+1) % (nx-1)] - u[(i-1) % (nx-1)])\n",
        "#     for i in range(nx):\n",
        "#         u[i] = u_new[i]\n",
        "#     if n % 10 == 0:\n",
        "#       tn = n*dt\n",
        "#       uexact = np.sin(-2 * np.pi * (x-tn) / L)\n",
        "#       plt.plot(x, u,'--',label='Numerical')\n",
        "#       plt.plot(x, uexact, label='Exact')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LH9nY4IALs7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}